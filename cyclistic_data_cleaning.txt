{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e11d218",
   "metadata": {},
   "source": [
    "# **Cyclistic Data Cleaning & Transformation Report**\n",
    "\n",
    "**By**: Roger Gomez  \n",
    "**Date**: March 25, 2025  \n",
    "**Project Context**:\n",
    "\n",
    "This report details the process of cleaning and transfoming the Cyclistic bike-share dataset, wich includes ride data from February 2024 to February 2025. The main objective is clean the data for further analysis of rider behavior, focusing on identifiying patters between annual members and casual riders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755a4ef2",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "**Overview:** This dataset contains information on bike rides recorded over a one-year period. The objetive of cleaning and transforming the data is to ensure accuracy and reliability, allowing for meaningful analysis and valuable insights.\n",
    "\n",
    "**Puporse:** Once cleaned, this data will provide insights into the behavior of riders who have used the company's services, helping to identify trends and patterns in their usage.\n",
    "\n",
    "**Importance of Data cleaning:** Porper data cleaning is essential for accurate analysis. It ensures that the data is free from inconsistencies, errors, and missing values, leading to more reliable and actionable insights. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaeffa0",
   "metadata": {},
   "source": [
    "## Data cleaning process \n",
    "\n",
    "In this section,I will outline the data cleanign workflow, detailing each step of the process in a structured and systematic manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e53ce",
   "metadata": {},
   "source": [
    "## Step 1: Removing Duplicates\n",
    "\n",
    "Let's work with `the ride_id` column. Since,`the ride_id` serves as a unique identifier for every rides, it's the best candidate for a Primary Key. Let's check if this columns contains any duplicates.\n",
    "\n",
    "Let's run the following query:\n",
    "\n",
    "\n",
    "---\n",
    "```sql \n",
    "SELECT ride_id, COUNT(*) AS time_repeated\n",
    "FROM cyclistic_data\n",
    "GROUP BY ride_id\n",
    "HAVING COUNT(*) > 1;\n",
    "```\n",
    "---\n",
    "\n",
    "This query identifies wheter any rows have duplicate values in the `ride_id` column. The output indicates that there are **211 duplicated rows**. Given this, my approach is to **remove the duplicates** and then convert `ride_id` into the **Pimary Key** to ensure data integrity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Removing duplicates Using a Window Function and CTE\n",
    "\n",
    "To remove duplicate rows based on the `ride_id` column, let's use a window function with a **Common Table Expression (CTE)**:\n",
    "\n",
    "---\n",
    "```sql\n",
    "WITH Duplicates AS (\n",
    "  SELECT *, \n",
    "         ROW_NUMBER() OVER (PARTITION BY ride_id ORDER BY ride_id) AS rn\n",
    "  FROM cyclistic_data\n",
    ")\n",
    "DELETE FROM cyclistic_data\n",
    "WHERE ride_id IN (\n",
    "    SELECT ride_id FROM Duplicates WHERE rn > 1\n",
    ");\n",
    "```\n",
    "---\n",
    "\n",
    "**Explanation**\n",
    "\n",
    "> The ``ROW_NUMBER()`` fuction assigns a unique row number ``rn`` to each row in the `ride_id` column.\n",
    "\n",
    ">``PARTITION BY ride_id`` ensures that row numbering restarts for each unique ``ride_id``\n",
    "\n",
    ">The `DELETE` statement removes all rows where `rn > 1 `,effectively keeping only the first occurence or each `ride_id`\n",
    "\n",
    "This approach ensures that only unique `ride_id` values remain in the dataset, improving data accuracy and consistency.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492c45e4",
   "metadata": {},
   "source": [
    "####  Handling Null Values in `ride_id` column\n",
    "\n",
    "Before, to create the primary key, let's check if the columns `ride_id` contains any `NULLs` value. \n",
    "\n",
    "Let's check if there is `NULLs` values by running this query:\n",
    "\n",
    "---\n",
    "```sql\n",
    "SELECT *\n",
    "FROM cyclistic_data_cleaned\n",
    "WHERE ride_id IS NULL;\n",
    "```\n",
    "---\n",
    "GREAT! It appears the columns does not have any `NULLs` values.\n",
    "\n",
    "#### Enforce the `Not Null` Constraint\n",
    "\n",
    "Even if no `NULL` values are present, we still need to **explicity modify the column definition** to enforce the ``NOT NULL``constraint. This is necessary because, by default, SQL Server might still allow `NULL` vlaues in `ride_id`\n",
    "\n",
    "---\n",
    "```sql\n",
    "ALTER TABLE cyclistic_data_cleaned\n",
    "ALTER COLUMN ride_id VARCHAR(50) NOT NULL;\n",
    "```\n",
    "---\n",
    "\n",
    "#### Creating the Primary Key\n",
    "\n",
    "Once we confirm that `ride_id` is unique and does not allow `NULL` values, we can safely create the **Primary Key**:\n",
    "\n",
    "---\n",
    "```sql\n",
    "ALTER TABLE cyclistic_data_clean\n",
    "ADD CONSTRAINT pk_ride PRIMARY KEY (ride_id);\n",
    "\n",
    "```\n",
    "---\n",
    "**Explanation** \n",
    "\n",
    ">The `ADD CONSTRAINT` command is used to add a new constraint to the table. The name of this constraint is `pk_ride`.\n",
    "\n",
    ">`PRIMARY KEY` command that works as a constrain, it enforces uniqueness and prevent Nulls value in the `ride_id` column.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e51aaa1",
   "metadata": {},
   "source": [
    "#### Confirming No Duplicates and Validating Primary Key \n",
    "\n",
    "To ensure there are no duplicates i nthe `ride_id` column, let's run this query once again:\n",
    "\n",
    "---\n",
    "```sql\n",
    "SELECT ride_id, COUNT(*) AS time_repeated\n",
    "FROM cyclistic_data\n",
    "GROUP BY ride_id\n",
    "HAVING COUNT(*) > 1;\n",
    "```\n",
    "---\n",
    "If the query returns no results, it confirms that the `ride_id` column contains **unique values**.\n",
    "\n",
    "Finally, let's see if SQL Server recognizes the `ride_id` column as a **Primary Key** by running the folowwing query:\n",
    "\n",
    "---\n",
    "```sql\n",
    "SELECT COLUMN_NAME\n",
    "FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE\n",
    "WHERE TABLE_NAME = 'cyclistic_data'\n",
    "  AND CONSTRAINT_NAME = 'PK_ride';\n",
    "```\n",
    "---\n",
    "The input will show the `ride_id` in `COLUMN_NAME`, confirming that it is indeed the **Primary Key**.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853975f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Handling `Nulls` values.\n",
    "\n",
    "`Nulls` can significantly affect the integrity of our data, pitentially leading to inaccurate insights. Identifying columns with `Nulls` values is an essential step in cleaning our data. Let's look any `Nulls` values in each column by using a **Dynamic SQL Query**\n",
    "\n",
    "#### Dynamic SQL Query for Identifying `Nulls` values\n",
    "\n",
    "First, I' ll write a dynamic query to check which columns contain `Nulls` values and how many `Nulls` are present in each column.\n",
    "\n",
    "Here's how we can can do this:\n",
    "\n",
    "---\n",
    "```sql\n",
    "DECLARE @nulls NVARCHAR(MAX) = '';\n",
    "```\n",
    "---\n",
    "\n",
    "**Explanation**\n",
    "\n",
    ">`DECLARE` initialzies a variable to store our dynamic query\n",
    "\n",
    ">`NVARCHAR(MAX)` is chosen as the type to accommodate potentially long queries\n",
    "\n",
    "\n",
    "\n",
    "Now, let's construct and execute the dynamuc query:\n",
    "\n",
    "---\n",
    "```sql\n",
    "SELECT @nulls = STRING_AGG(\n",
    "    'SELECT ''' + COLUMN_NAME + ''' AS column_name, COUNT(*) - COUNT(' + COLUMN_NAME + ') AS null_count FROM cyclistic_data', \n",
    "    ' UNION ALL ')  \t\t\t\t\t\t\t\t\t\n",
    "FROM INFORMATION_SCHEMA.COLUMNS \n",
    "WHERE TABLE_NAME = 'cyclistic_data';\n",
    "```\n",
    "---\n",
    "\n",
    "**Explanation**\n",
    "\n",
    ">`STRING_AGG` helps to concatenate the Dynamic SQL query parts for each column into one large query.\n",
    "\n",
    ">`+ COLUMN_NAME +`  Adds the columns dynamically inserted into each `SELECT` statement to check nulls in that specific column.\n",
    "\n",
    ">The query aggregates the results for all columns in the `cyclistic_data` table, calculating how many null values are present for each column.\n",
    "\n",
    ">`FROM INFORMATION_SCHEMA.COLUMNS ` lists all columns from all tables in our dataset `Cyclistic`\n",
    "dataset.\n",
    "\n",
    "`WHERE TABLE_NAME = 'cyclistic_data' ` filter the columns to only those in the `cyclistic_data` table.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Finalizing the Dynamic SQL Query\n",
    "\n",
    "After generating dynamic query to check for `NULL` values in each column, the next step is to refine the query be adding an `ORDER BY` clause to sort the columns base on the number of `NULL` values in descending order.\n",
    "\n",
    "Here's how can we can do that:\n",
    "\n",
    "---\n",
    "```[sql]\n",
    "SET @nulls = 'SELECT * FROM (' + @nulls + ') AS subquery ORDER BY null_count DESC;' ;\n",
    "```\n",
    "---\n",
    "\n",
    "**Explanation**\n",
    "\n",
    ">Since `STRING_AGG` works at the column level and cannot directly incorporate `ORDER BY` within itself, we use this approach to sort the results after aggregating them.\n",
    "\n",
    "\n",
    "Next, let's excecute the dynamically constructed query using;\n",
    "\n",
    "---\n",
    "```[sql]\n",
    "EXEC sp_executesql @nulls; \n",
    "```\n",
    "---\n",
    "\n",
    "**Explanation**\n",
    "\n",
    ">`EXEC sp_executesql @nulls;` This executes the dynamically created SQL query stored in the `nulls` variable.\n",
    "\n",
    "\n",
    "Great! The input shows that we have `Nulls` value for the next columns:\n",
    "\n",
    "`end_station_id`, `end_station_name`, `start_station_name`, `start_station_id`, `end_lng` and `end_lat`.\n",
    "\n",
    "After analyzing the context of the columns, I have decided to **preserve** the `NULLs` in the columns mentioned above.\n",
    "\n",
    "This decision was made to **preserve the integrity of the data**. Removing or replacing them with assumptions could potentially disort the analysis. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f74d0f",
   "metadata": {},
   "source": [
    "## Step 3: Working with Incorrect or Inaccurate Data\n",
    "\n",
    "Incorrect or inaccurate data can harm the integrity of the dataset and lead to misleading insights.\n",
    "\n",
    "In this step, I'll focus on the `start_ lat`, `start_lng`, `end_lat` and `end_lng` columns. Since our company operates in Chicago, **we need to ensure that the values in these columns are within the geographical boundaries of the city**\n",
    "\n",
    "> Chicago's Geographical Boundaries \n",
    "\n",
    "> Latitude: Chicagos's latitude ranges from 41.6째 to 42.1째.\n",
    "\n",
    "> Longitude: Chicago's longitude ranges from -88.0째 to -87.5째.\n",
    "\n",
    "Any values beyound these specific boundaries would be considered out of range, indicating inaccurate or incorrect data.\n",
    "\n",
    "\n",
    "I decided that the best approach is remove any rows with latitude or longitude values outside the defined range. This will protect the integrity of the dataset, ensuring all data points are valid and within the geographical bounds of Chicago.\n",
    "\n",
    "---\n",
    "```sql\n",
    "DELETE FROM cyclistic_data\n",
    "WHERE end_lat < 41.6 OR end_lat > 42.1\n",
    "   OR end_lng < -88.0 OR end_lng > -87.5\n",
    "   OR start_lat < 41.6 OR start_lat > 42.1\n",
    "   OR start_lng < -88.0 OR start_lng > -87.5;\n",
    "```\n",
    "---\n",
    "\n",
    "After running the query, **129 rows were affected,** confirming that there were 129 inacurrate rows in the dataset. By removing these rows, I can be confident that the remaining data points are withing Chicago's geographical boundaries.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d10edf",
   "metadata": {},
   "source": [
    "## Step 4: Transforming Data \n",
    "\n",
    "\n",
    "#### Converting to Spatial Data Type\n",
    "\n",
    "For can use the next columns for analysis, it needs to convert to a Spatial data type with the columns `start_lat`, `start_lng`, `end_lat` and `end_lng`.\n",
    "\n",
    "For do that, let's create new columns `start_location` and `end_location`.\n",
    "\n",
    "---\n",
    "```sql\n",
    "ALTER TABLE Cyclistic_data\n",
    "ADD start_location geography;\n",
    "```\n",
    "---\n",
    "---\n",
    "```sql\n",
    "ALTER TABLE Cyclistic_data\n",
    "ADD end_location geography;\n",
    "```\n",
    "---\n",
    "\n",
    "The `start_location` column will have the combination of `start_lat` and `start_lng` columns as a geographic points. The `end_location` will content the combination of `end_lat` and `end_lng` columns as geographic points.\n",
    "\n",
    "Now, let's update the `start_location` column by converting the `start_lat `and `start_lng` columns into a **geography point.** and do the same for the `end_location` by converting the `end_lat` and `end_lng`.\n",
    "\n",
    "---\n",
    "```sql\n",
    "UPDATE Cyclistic_data\n",
    "SET start_location = geography::Point(start_lat, start_lng, 4326)\n",
    "WHERE start_lat IS NOT NULL AND start_lng IS NOT NULL;\n",
    "```\n",
    "---\n",
    "---\n",
    "```sql\n",
    "UPDATE Cyclistic_data\n",
    "SET end_location = geography::Point(end_lat, end_lng, 4326)\n",
    "WHERE end_lat IS NOT NULL AND end_lng IS NOT NULL;\n",
    "```\n",
    "---\n",
    "\n",
    "*Note: 4326 is the SRID (Spatial Reference System Identifier) used for GPS coordinates in the WGS 84 coordinate system.*\n",
    "\n",
    "Great! We have all set.\n",
    "\n",
    "#### Creating `distance_meters` column\n",
    "\n",
    "This new column will calculate the distance between `start_location` to `end_location` in meters.\n",
    "\n",
    "First, let's create the `distance_meters` column\n",
    "\n",
    "---\n",
    "```sql\n",
    "ALTER TABLE Cyclistic_data  \n",
    "ADD distance_meters FLOAT;\n",
    "```\n",
    "---\n",
    "\n",
    "Then, let's use the function `STDistance` that calculate the distance between the columns in meters.\n",
    "\n",
    "---\n",
    "```sql\n",
    "UPDATE Cyclistic_data  \n",
    "SET distance_meters = ROUND(start_location.STDistance(end_location), 0);\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "#### Creating `minutes_traveled`  column\n",
    "\n",
    "For futher analysis, It would be nice if there's a column that tracks all the duration in minutes.\n",
    "\n",
    "Let's create that column and name it `minutes_traveled` \n",
    "\n",
    "\n",
    "---\n",
    "```sql\n",
    "ALTER TABLE Cyclistic_data \n",
    "ADD minutes_traveled INT ;\n",
    "```\n",
    "---\n",
    "\n",
    "Created the new column, let's update it with the trip duration in minutes.\n",
    "\n",
    "---\n",
    "```sql\n",
    "UPDATE Cyclistic_data  \n",
    "SET minutes_traveled = DATEDIFF(MINUTE, started_at, ended_at);\n",
    "```\n",
    "---\n",
    "\n",
    "#### Creating `avg_speed` column \n",
    "\n",
    "---\n",
    "```sql\n",
    "ALTER TABLE Cyclistic_data  \n",
    "ADD avg_speed_kmh FLOAT;\n",
    "\n",
    "```\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1924370",
   "metadata": {},
   "source": [
    "## Step 5:  Validating Categorical Data\n",
    "\n",
    "Another important step in data cleaning is to **ensure that categorical columns contain only valid values**. Incorrect or unexpected categories can introduce inconsistencies and affect the accuracy of analysis.\n",
    "\n",
    "#### Columns to Validate\n",
    "\n",
    "> `rideable_type`: This column should only contain three valid categories: `classic_bike`, `electric_bike` and `electric_scooter`\n",
    "\n",
    "\n",
    "The  `member_casual`: This column should only contain two valid categories: `casua` and `member`\n",
    "\n",
    "#### Validation Queries\n",
    "\n",
    "To check whether there are any unexpected values, let's run the following queries:\n",
    "\n",
    "---\n",
    "```sql\n",
    "SELECT \n",
    "\tDISTINCT(rideable_type)\n",
    "\t\n",
    "FROM cyclistic_data ;\n",
    "```\n",
    "---\n",
    "\n",
    "---\n",
    "```sql\n",
    "\n",
    "SELECT \n",
    "\tDISTINCT(member_casual)\n",
    "\t\n",
    "FROM cyclistic_data ;\n",
    "```\n",
    "---\n",
    "\n",
    "Great! the output confirmed that the columns only contain the expected categories. **No incorrect values were found,** meaning the data is clean in this regard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c025200",
   "metadata": {},
   "source": [
    "## Step 6: Validating and Cleaning Data & Time Data \n",
    "\n",
    "The `started_at` and `ended_at`columns contain timestamps that represent the start and end times of each ride. It is essential to validate their format and logical consistency before using them in our analysis.\n",
    "\n",
    "#### Cheking the Data Format\n",
    "\n",
    "Too ensure that the date format is correct, let's run the following query:\n",
    "\n",
    "---\n",
    "```sql\n",
    "SELECT TOP 10 started_at, ended_at\n",
    "FROM cyclistic_data;\n",
    "\n",
    "```\n",
    "---\n",
    "The format appears consistent, so we can proceed with further validations.\n",
    "\n",
    "\n",
    "#### Identifying and Removing Invalid Time Sequence\n",
    "\n",
    "A ride should not end before it starts, so let's check for case where `ended_at` is earlier than `started_at`\n",
    "\n",
    "----\n",
    "```sql\n",
    "SELECT \n",
    "started_at, ended_at \n",
    "FROM cyclistic_data\n",
    "WHERE ended_at < started_at\n",
    "```\n",
    "---\n",
    "\n",
    "According to the output **207 rows** where `ended_at` occurs before `started_at`.\n",
    "To maintain data integrity, let's remove these invalid records:\n",
    "\n",
    "\n",
    "---\n",
    "```sql\n",
    "DELETE FROM cyclistic_data\n",
    "WHERE ended_at < started_at ;\t\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "To confirm the deletion, let's run the following query:\n",
    "\n",
    "---\n",
    "```sql\n",
    "SELECT \n",
    "\tCOUNT(*)\n",
    "FROM cyclistic_data\n",
    "WHERE ended_at < started_at\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "**0 rows remaining** this issue is fully resolved\n",
    "\n",
    "\n",
    "#### Handling Ride Duration Outliers\n",
    "\n",
    "To remove rides that are **too short**(less than 1 minute) or **too long** (more than 24 hours), Let's execute the following query:\n",
    "\n",
    "---\n",
    "```sql\n",
    "DELETE FROM cyclistic_data\n",
    "WHERE DATEDIFF(MINUTE, started_at, ended_at) < 1\n",
    "   OR DATEDIFF(HOUR, started_at, ended_at) > 24;\n",
    "\n",
    "```\n",
    "---\n",
    "\n",
    "These outliers cases have been eliminated, ensuring only meaningful ride data remains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839914af",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Through a structured data cleaning process, we have significantly improved the quality and integrity of the **Cyclistic bike-share dataset**. Key steps included:\n",
    "\n",
    " **Removing Duplicates:** Ensured that each `ride_id` is unique, allowing us to establish it as the **primary key.**\n",
    "\n",
    " **Handling Nulls Values:** Retained `Nulls` is specific columns to preserve data integrity while ensuring essential were complete.\n",
    "\n",
    " **Correcting Inaccurate Data:** Eliminated rides with impossible start/end times and geographic coordinates outside of Chicago.\n",
    "\n",
    "**Standardizing Categories:** Verified that categorical fields contained only valid values.\n",
    "\n",
    " **Filtering Outliers:** Removed unrealistic ride durations to maintain data accuracy\n",
    "\n",
    "By applying these transformation, we now have a clean and reliable dataset that is ready for analysis. The next step will involve **exploring trends, patterns, and key insights** to better understand how annual members and casual riders use the bike-sharing service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c163d77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
